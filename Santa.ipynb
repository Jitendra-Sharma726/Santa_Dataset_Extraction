{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":88046,"databundleVersionId":10229277,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:16:45.635109Z","iopub.execute_input":"2024-11-27T20:16:45.636339Z","iopub.status.idle":"2024-11-27T20:16:46.936378Z","shell.execute_reply.started":"2024-11-27T20:16:45.636277Z","shell.execute_reply":"2024-11-27T20:16:46.935041Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import required libraries\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport seaborn as sns\n\n# Create the dataset from the provided data\ndata = {\n    \"id\": [0, 1, 2, 3, 4, 5],\n    \"text\": [\n        \"advent chimney elf family fireplace gingerbread mistletoe ornament reindeer scrooge\",\n        \"advent chimney elf family fireplace gingerbread mistletoe ornament reindeer scrooge walk give jump drive bake the sleep night laugh and\",\n        \"yuletide decorations gifts cheer holiday carol magi nutcracker polar grinch sleigh chimney workshop stocking ornament holly jingle beard naughty nice\",\n        \"yuletide decorations gifts cheer holiday carol magi nutcracker polar grinch sleigh chimney workshop stocking ornament holly jingle beard naughty nice sing cheer and of the is eat visit relax unwrap\",\n        \"hohoho candle poinsettia snowglobe peppermint eggnog fruitcake chocolate candy puzzle game doll toy workshop wonder believe dream hope peace joy merry season greeting card wrapping paper bow fireplace night cookie milk star wish wreath angel the to of and in that have it not with as you from we kaggle\",\n        \"advent chimney elf family fireplace gingerbread mistletoe ornament reindeer scrooge walk give jump drive bake the sleep night laugh and yuletide decorations gifts cheer holiday carol magi nutcracker polar grinch sleigh chimney workshop stocking ornament holly jingle beard naughty nice sing cheer and of the is eat visit relax unwrap hohoho candle poinsettia snowglobe peppermint eggnog fruitcake chocolate candy puzzle game doll toy workshop wonder believe dream hope peace joy merry season greeting card wrapping paper bow fireplace night cookie milk star wish wreath angel the to of and in that have it not with as you from we kaggle\"\n    ]\n}\n\n# Load the data into a Pandas DataFrame\ndf = pd.DataFrame(data)\n\n# View the dataset\nprint(\"Dataset:\")\nprint(df)\n\n# Combine all text into a single string for analysis\nall_text = \" \".join(df[\"text\"])\n\n# Split the text into words and count word frequencies\nword_list = all_text.split()\nword_counts = Counter(word_list)\n\n# Convert the word counts to a DataFrame\nword_freq_df = pd.DataFrame(word_counts.items(), columns=[\"Word\", \"Frequency\"]).sort_values(by=\"Frequency\", ascending=False)\n\n# Display the most common words\nprint(\"\\nMost Common Words:\")\nprint(word_freq_df.head(10))\n\n# Plot a bar chart of the top 10 words\nplt.figure(figsize=(10, 6))\nsns.barplot(x=\"Frequency\", y=\"Word\", data=word_freq_df.head(10), palette=\"viridis\")\nplt.title(\"Top 10 Most Frequent Words\", fontsize=16)\nplt.xlabel(\"Frequency\", fontsize=12)\nplt.ylabel(\"Word\", fontsize=12)\nplt.show()\n\n# Generate a Word Cloud\nwordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(all_text)\n\n# Display the Word Cloud\nplt.figure(figsize=(10, 6))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title(\"Word Cloud\", fontsize=16)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:18:21.514101Z","iopub.execute_input":"2024-11-27T20:18:21.514701Z","iopub.status.idle":"2024-11-27T20:18:23.378286Z","shell.execute_reply.started":"2024-11-27T20:18:21.514650Z","shell.execute_reply":"2024-11-27T20:18:23.377146Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from nltk.corpus import stopwords\nimport nltk\nnltk.download('stopwords')\n\n# Define stopwords\nstop_words = set(stopwords.words('english'))\n\n# Remove stopwords from the text\ndf[\"text_no_stopwords\"] = df[\"text\"].apply(\n    lambda x: \" \".join([word for word in x.split() if word.lower() not in stop_words])\n)\n\n# View the dataset after removing stopwords\nprint(\"\\nDataset without Stopwords:\")\nprint(df[[\"id\", \"text_no_stopwords\"]])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:21:16.202743Z","iopub.execute_input":"2024-11-27T20:21:16.203107Z","iopub.status.idle":"2024-11-27T20:21:16.214314Z","shell.execute_reply.started":"2024-11-27T20:21:16.203079Z","shell.execute_reply":"2024-11-27T20:21:16.213094Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate the number of words in each text entry\ndf[\"phrase_length\"] = df[\"text\"].apply(lambda x: len(x.split()))\n\n# View phrase lengths\nprint(\"\\nPhrase Lengths:\")\nprint(df[[\"id\", \"phrase_length\"]])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:21:47.655890Z","iopub.execute_input":"2024-11-27T20:21:47.656287Z","iopub.status.idle":"2024-11-27T20:21:47.665538Z","shell.execute_reply.started":"2024-11-27T20:21:47.656252Z","shell.execute_reply":"2024-11-27T20:21:47.664432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\n# Generate bigrams\nbigram_vectorizer = CountVectorizer(ngram_range=(2, 2))\nbigrams = bigram_vectorizer.fit_transform(df[\"text\"])\n\n# Convert to DataFrame\nbigram_df = pd.DataFrame(\n    bigrams.toarray(), columns=bigram_vectorizer.get_feature_names_out()\n).sum(axis=0).reset_index()\nbigram_df.columns = [\"Bigram\", \"Frequency\"]\nbigram_df = bigram_df.sort_values(by=\"Frequency\", ascending=False)\n\n# Display the top 10 bigrams\nprint(\"\\nTop 10 Bigrams:\")\nprint(bigram_df.head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:22:34.501929Z","iopub.execute_input":"2024-11-27T20:22:34.502363Z","iopub.status.idle":"2024-11-27T20:22:34.515952Z","shell.execute_reply.started":"2024-11-27T20:22:34.502325Z","shell.execute_reply":"2024-11-27T20:22:34.514648Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\n# Generate bigrams\nbigram_vectorizer = CountVectorizer(ngram_range=(2, 2))\nbigrams = bigram_vectorizer.fit_transform(df[\"text\"])\n\n# Convert to DataFrame\nbigram_df = pd.DataFrame(\n    bigrams.toarray(), columns=bigram_vectorizer.get_feature_names_out()\n).sum(axis=0).reset_index()\nbigram_df.columns = [\"Bigram\", \"Frequency\"]\nbigram_df = bigram_df.sort_values(by=\"Frequency\", ascending=False)\n\n# Display the top 10 bigrams\nprint(\"\\nTop 10 Bigrams:\")\nprint(bigram_df.head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:22:04.439564Z","iopub.execute_input":"2024-11-27T20:22:04.440042Z","iopub.status.idle":"2024-11-27T20:22:04.464326Z","shell.execute_reply.started":"2024-11-27T20:22:04.440002Z","shell.execute_reply":"2024-11-27T20:22:04.463217Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from textblob import TextBlob\n\n# Compute sentiment polarity\ndf[\"sentiment\"] = df[\"text\"].apply(lambda x: TextBlob(x).sentiment.polarity)\n\n# View sentiment scores\nprint(\"\\nSentiment Analysis:\")\nprint(df[[\"id\", \"sentiment\"]])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:22:52.006405Z","iopub.execute_input":"2024-11-27T20:22:52.006816Z","iopub.status.idle":"2024-11-27T20:22:52.099615Z","shell.execute_reply.started":"2024-11-27T20:22:52.006782Z","shell.execute_reply":"2024-11-27T20:22:52.098161Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# TF-IDF Vectorization\ntfidf_vectorizer = TfidfVectorizer(max_features=10)\ntfidf_matrix = tfidf_vectorizer.fit_transform(df[\"text\"])\n\n# Convert TF-IDF scores to a DataFrame\ntfidf_df = pd.DataFrame(\n    tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out()\n)\n\n# Add TF-IDF features to the original DataFrame\nfor col in tfidf_df.columns:\n    df[col] = tfidf_df[col]\n\nprint(\"\\nTF-IDF Features:\")\nprint(df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:23:11.357493Z","iopub.execute_input":"2024-11-27T20:23:11.357954Z","iopub.status.idle":"2024-11-27T20:23:11.382289Z","shell.execute_reply.started":"2024-11-27T20:23:11.357918Z","shell.execute_reply":"2024-11-27T20:23:11.381281Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# TF-IDF Vectorization\ntfidf_vectorizer = TfidfVectorizer(max_features=10)\ntfidf_matrix = tfidf_vectorizer.fit_transform(df[\"text\"])\n\n# Convert TF-IDF scores to a DataFrame\ntfidf_df = pd.DataFrame(\n    tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out()\n)\n\n# Add TF-IDF features to the original DataFrame\nfor col in tfidf_df.columns:\n    df[col] = tfidf_df[col]\n\nprint(\"\\nTF-IDF Features:\")\nprint(df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:23:32.339556Z","iopub.execute_input":"2024-11-27T20:23:32.340003Z","iopub.status.idle":"2024-11-27T20:23:32.357527Z","shell.execute_reply.started":"2024-11-27T20:23:32.339968Z","shell.execute_reply":"2024-11-27T20:23:32.356362Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\n\n# Tokenize text into words\ndf[\"tokens\"] = df[\"text\"].apply(lambda x: word_tokenize(x))\n\n# View tokenized text\nprint(\"\\nTokenized Text:\")\nprint(df[[\"id\", \"tokens\"]])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:23:52.531968Z","iopub.execute_input":"2024-11-27T20:23:52.532375Z","iopub.status.idle":"2024-11-27T20:23:52.556570Z","shell.execute_reply.started":"2024-11-27T20:23:52.532341Z","shell.execute_reply":"2024-11-27T20:23:52.555503Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Count unique words\ndf[\"unique_words\"] = df[\"text\"].apply(lambda x: len(set(x.split())))\n\n# View unique word counts\nprint(\"\\nUnique Words Count:\")\nprint(df[[\"id\", \"unique_words\"]])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:24:12.852882Z","iopub.execute_input":"2024-11-27T20:24:12.853331Z","iopub.status.idle":"2024-11-27T20:24:12.863287Z","shell.execute_reply.started":"2024-11-27T20:24:12.853297Z","shell.execute_reply":"2024-11-27T20:24:12.862165Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the updated dataset\ndf.to_csv(\"processed_text_data.csv\", index=False)\nprint(\"\\nProcessed data saved to 'processed_text_data.csv'.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:24:29.221372Z","iopub.execute_input":"2024-11-27T20:24:29.221825Z","iopub.status.idle":"2024-11-27T20:24:29.237468Z","shell.execute_reply.started":"2024-11-27T20:24:29.221788Z","shell.execute_reply":"2024-11-27T20:24:29.236120Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.histplot(df[\"phrase_length\"], bins=10, kde=True, color=\"blue\")\nplt.title(\"Phrase Length Distribution\", fontsize=16)\nplt.xlabel(\"Number of Words\", fontsize=12)\nplt.ylabel(\"Frequency\", fontsize=12)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:24:59.602612Z","iopub.execute_input":"2024-11-27T20:24:59.603019Z","iopub.status.idle":"2024-11-27T20:24:59.941390Z","shell.execute_reply.started":"2024-11-27T20:24:59.602987Z","shell.execute_reply":"2024-11-27T20:24:59.940314Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.histplot(df[\"phrase_length\"], bins=10, kde=True, color=\"blue\")\nplt.title(\"Phrase Length Distribution\", fontsize=16)\nplt.xlabel(\"Number of Words\", fontsize=12)\nplt.ylabel(\"Frequency\", fontsize=12)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:25:17.242565Z","iopub.execute_input":"2024-11-27T20:25:17.243696Z","iopub.status.idle":"2024-11-27T20:25:17.566577Z","shell.execute_reply.started":"2024-11-27T20:25:17.243605Z","shell.execute_reply":"2024-11-27T20:25:17.565453Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"filtered_text = \" \".join(df[\"text_no_stopwords\"])\nwordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(filtered_text)\n\nplt.figure(figsize=(10, 6))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title(\"Word Cloud without Stopwords\", fontsize=16)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:25:39.434194Z","iopub.execute_input":"2024-11-27T20:25:39.434559Z","iopub.status.idle":"2024-11-27T20:25:40.045765Z","shell.execute_reply.started":"2024-11-27T20:25:39.434528Z","shell.execute_reply":"2024-11-27T20:25:40.044551Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"filtered_text = \" \".join(df[\"text_no_stopwords\"])\nwordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(filtered_text)\n\nplt.figure(figsize=(10, 6))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title(\"Word Cloud without Stopwords\", fontsize=16)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:25:55.858860Z","iopub.execute_input":"2024-11-27T20:25:55.859261Z","iopub.status.idle":"2024-11-27T20:25:56.477898Z","shell.execute_reply.started":"2024-11-27T20:25:55.859227Z","shell.execute_reply":"2024-11-27T20:25:56.476753Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import nltk\nnltk.download('averaged_perceptron_tagger')\n\n# POS tagging\ndf[\"pos_tags\"] = df[\"text\"].apply(lambda x: nltk.pos_tag(nltk.word_tokenize(x)))\n\n# View POS tagging\nprint(\"\\nPart-of-Speech Tags:\")\nprint(df[[\"id\", \"pos_tags\"]])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:28:31.654772Z","iopub.execute_input":"2024-11-27T20:28:31.655936Z","iopub.status.idle":"2024-11-27T20:28:31.873272Z","shell.execute_reply.started":"2024-11-27T20:28:31.655867Z","shell.execute_reply":"2024-11-27T20:28:31.871845Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import spacy\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Named Entity Recognition\ndf[\"entities\"] = df[\"text\"].apply(lambda x: [(ent.text, ent.label_) for ent in nlp(x).ents])\n\n# View entities\nprint(\"\\nNamed Entities:\")\nprint(df[[\"id\", \"entities\"]])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:29:02.278556Z","iopub.execute_input":"2024-11-27T20:29:02.279498Z","iopub.status.idle":"2024-11-27T20:29:08.398197Z","shell.execute_reply.started":"2024-11-27T20:29:02.279457Z","shell.execute_reply":"2024-11-27T20:29:08.397142Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def jaccard_similarity(text1, text2):\n    set1 = set(text1.split())\n    set2 = set(text2.split())\n    return len(set1.intersection(set2)) / len(set1.union(set2))\n\n# Compute similarity between all texts\ndf[\"similarity_with_first\"] = df[\"text\"].apply(lambda x: jaccard_similarity(x, df[\"text\"][0]))\n\n# View similarity scores\nprint(\"\\nJaccard Similarity with First Text:\")\nprint(df[[\"id\", \"similarity_with_first\"]])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:31:57.927832Z","iopub.execute_input":"2024-11-27T20:31:57.928802Z","iopub.status.idle":"2024-11-27T20:31:57.941287Z","shell.execute_reply.started":"2024-11-27T20:31:57.928749Z","shell.execute_reply":"2024-11-27T20:31:57.939961Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\n\n# Compute cosine similarity\nsimilarity_matrix = cosine_similarity(tfidf_matrix)\n\n# Convert to DataFrame for visualization\nsimilarity_df = pd.DataFrame(similarity_matrix, columns=df[\"id\"], index=df[\"id\"])\n\n# Display similarity matrix\nprint(\"\\nTF-IDF Similarity Matrix:\")\nprint(similarity_df)\n\n# Heatmap of similarity\nplt.figure(figsize=(8, 6))\nsns.heatmap(similarity_df, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\nplt.title(\"Text Similarity (TF-IDF)\", fontsize=16)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:32:18.241638Z","iopub.execute_input":"2024-11-27T20:32:18.242093Z","iopub.status.idle":"2024-11-27T20:32:18.678325Z","shell.execute_reply.started":"2024-11-27T20:32:18.242058Z","shell.execute_reply":"2024-11-27T20:32:18.677073Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.lineplot(x=df[\"id\"], y=df[\"sentiment\"], marker=\"o\", color=\"purple\")\nplt.title(\"Sentiment Trend\", fontsize=16)\nplt.xlabel(\"Text ID\", fontsize=12)\nplt.ylabel(\"Sentiment Polarity\", fontsize=12)\nplt.axhline(0, color=\"red\", linestyle=\"--\", linewidth=1)  # Neutral line\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:32:36.682406Z","iopub.execute_input":"2024-11-27T20:32:36.682950Z","iopub.status.idle":"2024-11-27T20:32:37.028519Z","shell.execute_reply.started":"2024-11-27T20:32:36.682890Z","shell.execute_reply":"2024-11-27T20:32:37.027383Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from nltk.corpus import wordnet\n\n# Extract synonyms for each word\ndef find_synonyms(text):\n    synonyms = {}\n    for word in text.split():\n        syns = wordnet.synsets(word)\n        if syns:\n            synonyms[word] = list(set([syn.lemmas()[0].name() for syn in syns]))\n    return synonyms\n\ndf[\"synonyms\"] = df[\"text\"].apply(find_synonyms)\n\n# View synonyms\nprint(\"\\nSynonyms:\")\nprint(df[[\"id\", \"synonyms\"]])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:32:56.466089Z","iopub.execute_input":"2024-11-27T20:32:56.466901Z","iopub.status.idle":"2024-11-27T20:32:56.638988Z","shell.execute_reply.started":"2024-11-27T20:32:56.466862Z","shell.execute_reply":"2024-11-27T20:32:56.637366Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}